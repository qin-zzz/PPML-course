{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy Preserving Machine Learning\n",
    "\n",
    "Course taught by Aur√©lien Bellet\n",
    "\n",
    "Course page: http://researchers.lille.inria.fr/abellet/teaching/private_machine_learning_course.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical session 4: Differentially Private SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You are free to work with any binary classification dataset(s) you like** (you may use more than a single dataset). It is of course possible to work with the US Census dataset used in previous practicals, but you can find other datasets for instance on [OpenML](https://www.openml.org/), [UCI](https://archive.ics.uci.edu/ml/index.php), [sklearn](https://scikit-learn.org/stable/modules/classes.html?highlight=datasets#module-sklearn.datasets).\n",
    "\n",
    "Good candidate datasets should have rather small dimension compared to the number of data points. Examples include US Census in one-hot encoded version (`name='a9a', version=1`), houses (`name='houses', version=2`) and electricity (`name='electricity', version=1`).\n",
    "\n",
    "The code below loads the US Census dataset in one-hot encoded version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48842 123\n"
     ]
    }
   ],
   "source": [
    "X, y = fetch_openml(name='a9a', version=1, return_X_y=True, as_frame=False)\n",
    "n, d = X.shape\n",
    "\n",
    "# convert labels to -1, 1\n",
    "c = np.unique(y)    \n",
    "y[y==c[0]] = -1\n",
    "y[y==c[1]] = 1\n",
    "y = y.astype(float)\n",
    "\n",
    "print(n, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first standardize features, then normalize each point to have unit norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sp.sparse.issparse(X):\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "normalizer = Normalizer()\n",
    "X = normalizer.transform(scaler.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the dataset into a train and a test set. Feel free to adapt the size of the training set to your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39073, 123) (39073,) (9769, 123) (9769,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=42, stratify=y)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "n_train = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (non-private SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first question, we will implement our own simple version of SGD, as well as define our own sklearn-compatible $\\ell_2$-regularized logistic regression estimator. This will be convenient when we will implement a differentially private version in Question 2.\n",
    "\n",
    "Below, you are given several pieces of code:\n",
    "1. A function `sgd` which implements SGD: it is meant to be generic in the sense that it takes as input a function `obj_and_grad` which computes the value and the gradient of the desired objective function. **This function has missing parts that you need to complete**.\n",
    "2. A function `my_logistic_obj_and_grad` (adapted from [the version from sklearn](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf39bbdacd6ed713c00724f8f871d60370/sklearn/linear_model/_logistic.py#L84)) which computes the value and gradient of the logistic regression problem. You do not need to modify this function.\n",
    "3. A class `MySGDLogisticRegression` which defines a sklearn estimator for logistic regression, where the model is fit using SGD using the previous two functions. You do not need to modify this function.\n",
    "\n",
    "Spend a bit of time to get familiar with the code provided, then complete the missing bits in the `sgd` function. Make sure it works by trying it on the binary classification dataset that you previously loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X, y, gamma, n_iter, obj_and_grad, theta_init, n_batch=1, freq_obj_eval=10,\n",
    "        n_obj_eval=1000, random_state=None):\n",
    "    \"\"\"Stochastic Gradient Descent (SGD) algorithm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n, d)\n",
    "        The data\n",
    "    y : array, shape (n,)\n",
    "        Binary labels (-1, 1).\n",
    "    gamma : float | callable\n",
    "        The step size. Can be a constant float or a function\n",
    "        that allows to have a variable step size\n",
    "    n_iter : int\n",
    "        The number of iterations\n",
    "    obj_and_grad : callable\n",
    "        A function which takes as a vector of shape (p,), a dataset of shape (n_batch, d)\n",
    "        and a label vector of shape (n_batch,), and returns the objective value and gradient.\n",
    "    theta_init : array, shape (p,)\n",
    "        The initial value for the model parameters\n",
    "    n_batch : int\n",
    "        Size of the mini-batch to use at each iteration of SGD.\n",
    "    freq_obj_eval : int\n",
    "        Specifies the frequency (in number of iterations) at which we compute the objective\n",
    "    n_obj_eval : int\n",
    "        The number of points on which we evaluate the objective\n",
    "    random_state : int\n",
    "        Random seed to make the algorithm deterministic\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    theta : array, shape=(p,)\n",
    "        The final value of the model parameters\n",
    "    obj_list : list of length (n_iter / freq_obj_eval)\n",
    "        A list containing the value of the objective function computed every freq_obj_eval iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n, d = X.shape\n",
    "    p = theta_init.shape[0]\n",
    "    \n",
    "    theta = theta_init.copy()\n",
    "\n",
    "    # if a constant step size was provided, we turn it into a constant function\n",
    "    if not callable(gamma):\n",
    "        def gamma_func(t):\n",
    "            return gamma\n",
    "    else:\n",
    "        gamma_func = gamma\n",
    "    \n",
    "    # list to record the evolution of the objective (for plotting)\n",
    "    obj_list = []\n",
    "    # we draw a fixed subset of points to monitor the objective\n",
    "    idx_eval = rng.randint(0, n, n_obj_eval)\n",
    "\n",
    "    for t in range(n_iter):\n",
    "        if t % freq_obj_eval == 0:\n",
    "            # evaluate objective\n",
    "            obj, _ = obj_and_grad(theta, X[idx_eval, :], y[idx_eval])\n",
    "            obj_list.append(obj)\n",
    "        \n",
    "        # TO COMPLETE\n",
    "        \n",
    "    return theta, obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model._base import LinearClassifierMixin, SparseCoefMixin, BaseEstimator\n",
    "from sklearn.utils.extmath import log_logistic, safe_sparse_dot\n",
    "from sklearn.linear_model._logistic import _intercept_dot\n",
    "from scipy.special import expit\n",
    "from sklearn.utils.validation import check_X_y\n",
    "\n",
    "def my_logistic_obj_and_grad(theta, X, y, lamb):\n",
    "    \"\"\"Computes the value and gradient of the objective function of logistic regression defined as:\n",
    "    min (1/n) \\sum_i log_loss(theta;X[i,:],y[i]) + (lamb / 2) \\|w\\|^2,\n",
    "    where theta = w (if no intercept), or theta = [w b] (if intercept)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta_init : array, shape (d,) or (d+1,)\n",
    "        The initial value for the model parameters. When an intercept is used, it corresponds to the last entry\n",
    "    X : array, shape (n, d)\n",
    "        The data\n",
    "    y : array, shape (n,)\n",
    "        Binary labels (-1, 1)\n",
    "    lamb : float\n",
    "        The L2 regularization parameter\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    obj : float\n",
    "        The value of the objective function\n",
    "    grad : array, shape (d,) or (d+1,)\n",
    "        The gradient of the objective function\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    grad = np.empty_like(theta)\n",
    "\n",
    "    w, c, yz = _intercept_dot(theta, X, y)\n",
    "\n",
    "    # Logistic loss is the negative of the log of the logistic function\n",
    "    obj = -np.mean(log_logistic(yz)) + .5 * lamb * np.dot(w, w)\n",
    "\n",
    "    z = expit(yz)\n",
    "    z0 = (z - 1) * y\n",
    "\n",
    "    grad[:n_features] = safe_sparse_dot(X.T, z0) / n_samples + lamb * w\n",
    "\n",
    "    # Case where we fit the intercept\n",
    "    if grad.shape[0] > n_features:\n",
    "        grad[-1] = z0.sum() / n_samples\n",
    "    return obj, grad\n",
    "\n",
    "\n",
    "class MySGDLogisticRegression(BaseEstimator, LinearClassifierMixin, SparseCoefMixin):\n",
    "    \"\"\"Our own sklearn estimator for logistic regression defined as:\n",
    "    min (1/n) \\sum_i log_loss(theta;X[i,:],y[i]) + (lamb / 2) \\|w\\|^2,\n",
    "    where theta = [w b]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma : float | callable\n",
    "        The step size. Can be a constant float or a function\n",
    "        that allows to have a variable step size\n",
    "    n_iter : int\n",
    "        The number of iterations\n",
    "    lamb : float\n",
    "        The L2 regularization parameter\n",
    "    n_batch : int\n",
    "        Size of the mini-batch to use at each iteration of SGD.\n",
    "    freq_obj_eval : int\n",
    "        Specifies the frequency (in number of iterations) at which we compute the objective\n",
    "    n_obj_eval : int\n",
    "        The number of points on which we evaluate the objectuve\n",
    "    random_state : int\n",
    "        Random seed to make the algorithm deterministic\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    coef_ : (p,)\n",
    "        The weights of the logistic regression model.\n",
    "    intercept_ : (1,)\n",
    "        The intercept term of the logistic regression model.\n",
    "    obj_list_: list of length (n_iter / freq_obj_eval)\n",
    "        A list containing the value of the objective function computed every freq_loss_eval iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma, n_iter, lamb=0, n_batch=1, freq_obj_eval=10, n_obj_eval=1000, random_state=None):\n",
    "        self.gamma = gamma\n",
    "        self.n_iter = n_iter\n",
    "        self.lamb = lamb\n",
    "        self.n_batch = n_batch\n",
    "        self.freq_obj_eval = freq_obj_eval\n",
    "        self.n_obj_eval = n_obj_eval\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # WARNING: assumes labels are -1, 1\n",
    "        X, y = check_X_y(X, y, accept_sparse='csr', dtype=[np.float64, np.float32], order=\"C\")\n",
    "        self.classes_ = np.unique(y)    \n",
    "                \n",
    "        p = X.shape[1]\n",
    "        theta_init = np.zeros(p+1) # initialize parameters to zero\n",
    "        # define the function for value and gradient needed by SGD\n",
    "        obj_grad = lambda theta, X, y: my_logistic_obj_and_grad(theta, X, y, lamb=self.lamb)\n",
    "        theta, obj_list = sgd(X, y, self.gamma, self.n_iter, obj_grad, theta_init, self.n_batch,\n",
    "                              self.freq_obj_eval, self.n_obj_eval, self.random_state)\n",
    "        \n",
    "        # save the learned model into the appropriate quantities used by sklearn\n",
    "        self.intercept_ = np.expand_dims(theta[-1], axis=0)\n",
    "        self.coef_ = np.expand_dims(theta[:-1], axis=0)\n",
    "        \n",
    "        # also save list of objective values during optimization for plotting\n",
    "        self.obj_list_ = obj_list\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.7606715119254785\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lamb = 0\n",
    "n_iter = 5000\n",
    "n_batch = 1\n",
    "gamma = 0.05\n",
    "# gamma = lambda t: 1 / np.sqrt(t)\n",
    "\n",
    "mlr = MySGDLogisticRegression(gamma, n_iter, lamb, n_batch=n_batch, random_state=None)\n",
    "mlr.fit(X_train, y_train)\n",
    "print(\"Test accuracy\", mlr.score(X_test, y_test))\n",
    "\n",
    "obj_list = mlr.obj_list_\n",
    "iter_list = np.arange(len(obj_list)) * mlr.freq_obj_eval\n",
    "plt.plot(iter_list, obj_list)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Objective function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (private SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement and experiment with DP-SGD:\n",
    "1. Following the model of the function `sgd`, implement a new function `private_sgd` which implements DP-SGD **with mini-batch size of 1 and no regularization**. It can take as input the desired value of $\\epsilon$ and $\\delta$ for the $(\\epsilon,\\delta)$-DP, or alternatively the standard deviation of the Gaussian noise to add at each iteration. Note: you do not need to make the objective plotting part private (this is only for monitoring).\n",
    "2. Following the model of the class `MySGDLogisticRegression`, implement a new class `MyPrivateSGDLogisticRegression` which implements differentially private logistic regression trained using your DP-SGD implementation above.\n",
    "3. Experiment with different values of $\\epsilon$ and $\\delta$, number of iterations and step size, and study the effect on the convergence of SGD as well as the test accuracy of the resulting model. Describe your observations. How should the number of iterations depend on the level of privacy? How can we choose the number of iterations and step size in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (extending the scope of private SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the following extensions, which were briefly discussed in the lecture:\n",
    "1. *$\\ell_2$-regularization*: What is the sensitivity of the stochastic gradient when adding $\\ell_2$-regularization to the objective (parameter `lamb` in the code above) ? Adapt your code if needed.\n",
    "2. *Mini-batch*: What is the sensitivity of a stochastic gradient when it is evaluated on a mini-batch of $b$ data points (parameter `n_batch` in the code above) ? Adapt your code if needed.\n",
    "3. *Gradient clipping*: If the loss function $L$ is not Lipschitz, or when the Lipschitz constant is difficult to bound, the idea of gradient clipping consists in rescaling each individual gradient that have a norm larger than some constant $C$ to have norm equal to $C$:\n",
    "$$\\text{clip}(\\nabla L(\\theta;x,y), C) = \\min\\Big(1,\\frac{C}{\\|\\nabla L(\\theta;x,y)\\|_2}\\Big)\\nabla L(\\theta;x,y)$$\n",
    "\n",
    "Explain how this allows to bound the gradient sensitivity without any assumption on the Lipschitzness of the loss. Implement this variant in a function `private_sgd_with_clipping` and explore how to choose the value of $C$ for logistic regression on the *unnormalized* version of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question 1 (DP-SGD for linear SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply DP-SGD on linear SVM, which is obtained by replacing the logistic loss by the hinge loss $L(w,b;x,y)=\\max(0, 1-y(w^Tx + b))$. For this, you need to implement a function `my_hinge_obj_and_grad` which computes the value and gradient of the objective function. Note that the hinge loss is differentiable everywhere, except at $0$ where it is sub-differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question 2 (DP-SGD for deep learning with TensorFlow Privacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TensorFlow Privacy](https://github.com/tensorflow/privacy) contains implementations of TensorFlow optimizers for training machine learning models with differential privacy, in particular DP-SGD and its variants. You can install it by following the instructions given on GitHub (it is possible to install directly with `pip`).\n",
    "\n",
    "Use TF Privacy to train a differentially private deep learning model on one of the image datasets provided in TF. To get familiar with TF Privacy, you may follow [this tutorial](https://github.com/tensorflow/privacy/blob/master/tutorials/walkthrough/README.md). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
