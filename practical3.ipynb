{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy Preserving Machine Learning\n",
    "\n",
    "Course taught by Aur√©lien Bellet\n",
    "\n",
    "Course page: http://researchers.lille.inria.fr/abellet/teaching/private_machine_learning_course.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical session 3: Differentially Private ERM via Output Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions for submitting your report\n",
    "\n",
    "<font color=red>The deadline for sending your report is **Monday, November 21, 2022 at 23h55**.\n",
    "The report is *personal*, should only consist of a *single* `ipynb` file (Jupyter notebook), and be sent to me by email (`aurelien[dot]bellet[at]inria[dot]fr`).\n",
    "\n",
    "The grade will be over 20 points, broken down as follows:\n",
    "- Quality of your answers to the questions: **17** points\n",
    "- Quality of the writing and presentation: **2** points\n",
    "- Absence of any bug: **1** point\n",
    "\n",
    "Penalties: **5** points per 12 hours of extra time; **2** points for any failure to respect the other instructions above.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You are free to work with any binary classification dataset(s) you like** (you may use more than a single dataset).\n",
    "\n",
    "It is of course possible to work with the US Census dataset used in previous practicals, but some of the observations may more notable on higher dimensional datasets. You can find datasets for instance on [OpenML](https://www.openml.org/), [UCI](https://archive.ics.uci.edu/ml/index.php), [sklearn](https://scikit-learn.org/stable/modules/classes.html?highlight=datasets#module-sklearn.datasets).\n",
    "\n",
    "The code below loads a version of the Arcene dataset, which aims to distinguish cancer versus healthy patients from mass-spectrometric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 10000\n"
     ]
    }
   ],
   "source": [
    "X, y = fetch_openml(name='Arcene', version=1, return_X_y=True, as_frame=False)\n",
    "features_complete = (np.isnan(X).sum(axis=0)) == 0 # features without missing values\n",
    "X = X[:, features_complete]\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X) # standardize features\n",
    "n, d = X.shape\n",
    "print(n, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (non-private training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with $\\ell_2$-regularized logistic regression. For now, let us consider the non-private setting.\n",
    "\n",
    "1. What is the Lipschitz constant $L$ of the logistic loss for a data point $(x,y)$? *(Note: we have seen this in the lecture)*. Normalize the dataset to ensure that the logistic loss is 1-Lipschitz for any $(x,y)$.\n",
    "2. For different splits of the dataset into train (80%) and test (20%) sets, plot the accuracy of an $\\ell_2$-regularized logistic regression classifier on the test set with respect to the size of the training set. To do this, you can use the `plot_learning_curve` function below, setting its `cv` argument to a `ShuffleSplit` instance (see [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curve(estimator, X, y, cv, train_sizes=np.linspace(0.1, 1.0, 5)):\n",
    "    \"\"\"Learning curve.\n",
    "\n",
    "    Determines cross-validated training and test scores for different training\n",
    "    set sizes.\n",
    "\n",
    "    A cross-validation generator splits the whole dataset k times in training\n",
    "    and test data. Subsets of the training set with varying sizes will be used\n",
    "    to train the estimator and a score for each training subset size and the\n",
    "    test set will be computed. Afterwards, the scores will be averaged over\n",
    "    all k runs for each training subset size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    cv : cross-validation generator\n",
    "        Cross-validation object, see sklearn.model_selection module\n",
    "        for the list of possible objects\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float\n",
    "        Relative numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_sizes_abs : array, shape (n_unique_ticks,), dtype int\n",
    "        Numbers of training examples that has been used to generate the\n",
    "        learning curve. Note that the number of ticks might be less\n",
    "        than n_ticks because duplicate entries will be removed.\n",
    "\n",
    "    train_scores : array, shape (n_ticks, n_cv_folds)\n",
    "        Scores on training sets.\n",
    "\n",
    "    test_scores : array, shape (n_ticks, n_cv_folds)\n",
    "        Scores on test set.\n",
    "    \"\"\"\n",
    "\n",
    "    n_max_training_samples = len(list(cv.split(X))[0][0])\n",
    "    train_sizes_abs = (train_sizes * n_max_training_samples).astype(int)\n",
    "\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "\n",
    "    for train, test in cv.split(X):\n",
    "        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n",
    "        this_train_scores = []\n",
    "        this_test_scores = []\n",
    "        for n in train_sizes_abs:\n",
    "            estimator.fit(X_train[:n], y_train[:n])\n",
    "            this_train_scores.append(estimator.score(X_train[:n], y_train[:n]))\n",
    "            this_test_scores.append(estimator.score(X_test, y_test))\n",
    "\n",
    "        train_scores.append(this_train_scores)\n",
    "        test_scores.append(this_test_scores)\n",
    "\n",
    "    return train_sizes_abs, np.transpose(train_scores), np.transpose(test_scores)\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, X, y, title=\"\", ylim=None, cv=None,\n",
    "                        train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"Generate a simple plot of the test (or validation) and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : integer, cross-validation generator, optional\n",
    "        If an integer is passed, it is the number of folds (defaults to 3).\n",
    "        Specific cross-validation objects can be passed, see\n",
    "        sklearn.cross_validation module for the list of possible objects\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Test score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (sensitivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the $\\ell_2$ regularized logistic regression solves the following problem:\n",
    "$$\\arg\\min_{\\theta\\in\\mathbb{R}^p} \\frac{1}{n}\\sum_{i=1}^n \\log(1+e^{-y\n",
    "(\\theta^\\top x)}) + \\lambda\\|\\theta\\|_2^2$$\n",
    "\n",
    "1. What is the theoretical upper-bound on the $\\ell_2$-sensitivity of the above formulation, based on the result seen in the lecture?\n",
    "2. We would like to see how large the sensitivity is in practice when we train a logistic regression classifier on two neighboring datasets. Let `n_eval` be the training set size and `n_runs` the number of times we would like to repeat the experiment. For each of the `n_runs` experiments, you should randomly generate a pair of neighboring datasets $D,D'$ of size `n_eval` which differ on a single data point, train logistic regression on $D$ and $D'$, and compute the sensitivity you observe empirically. Implement this procedure as a function.\n",
    "3. Generate one train/test split. For `n_eval=100`, `n_runs=100` and a value $\\lambda$ of your choice, use the function you implemented to compare the \"empirical sensitivity\" you observe to the theoretical bound. Explain any discrepancy between the two. **Warning**: scikit-learn solves the logistic regression problem in a slighly different form, see the [documentation](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression). You should make sure that you appropriately convert the value $\\lambda$ in the optimization problem above into a value for the parameter `C` of the implementation so that both problems are equivalent.\n",
    "4. **(bonus)** Investigate what the differing data points (both features and labels) look like in the \"best\" and \"worst\" case values of the empirical sensitivity. Investigate how the discrepancy between the empirical and theoretical sensitivity evolves with `n_eval` and $\\lambda$. Briefly describe your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (private training via output perturbation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that you hold the private dataset and you would like to publicly release a logistic regression classifier trained on private dataset in a differentially private manner. We will do this with the DP-ERM via output perturbation approach seen in the lecture.\n",
    "\n",
    "1. Do you think it is safe to use an empirical estimate of the sensitivity (e.g., as computed in Question 2) instead of a theoretical upper-bound to calibrate the noise? Justify your answer.\n",
    "2. Implement the DP-ERM via output perturbation approach. The function should take as input a (private) training dataset, a value of regularization parameter $\\lambda$, and privacy parameters $\\epsilon$ and $\\delta$, and return a model trained on the private dataset in an $(\\epsilon,\\delta)$-DP manner.\n",
    "3. Experiment with your DP-ERM implementation for different training set size, regularization parameter and privacy parameter $\\epsilon$ (you can set $\\delta$ to a fixed value by following the recommendation seen in the second lecture). Compare the test accuracy of the resulting model to the non-private case. Discuss the trade-offs you obtain. Note: it may be convenient to make your DP-ERM implementation a [proper sklearn estimator](https://scikit-learn.org/stable/developers/develop.html#rolling-your-own-estimator) so you can reuse the `plot_learning_curve` function from Question 1.\n",
    "4. **(bonus)** As discussed in the lecture, the cost of differential privacy for machine learning can be measured as the number of additional data points needed to match the test accuracy of the non-private case. Illustrate this with an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (membership inference attack)\n",
    "\n",
    "Machine learning models can leak information about the individual data points on which they were trained. Given a data point and some access to target model (e.g., to its parameters or only to predictions made by the model), a *membership inference attack* aims to predict whether the data point was in the model's training dataset.\n",
    "\n",
    "Here, we consider the simplest setting for membership inference attacks, where the attacker has access to the parameters of the trained model (\"white-box\") as well as a separate set of data points from the same distribution as the training set (you may use the test set of previous questions for this purpose).\n",
    "\n",
    "We will consider a simple membership inference attack which uses the (logistic) loss of the target model on a given point to predict whether this point was in the training dataset. The logistic loss can be computed from the soft prediction (obtained from the method `predict_proba`) using `sklearn.metrics.log_loss`, see [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html).\n",
    "\n",
    "1. Find the threshold on the loss that would yield the best accuracy for this attack, and report the corresponding accuracy.  \n",
    "2. Recall that the attacker does not have access to the training set, so it cannot find the optimal threshold as done in the previous question. Playing the role of an attacker with access to a target model and a separate set of points (but not the training set!), implement a feasible membership inference attack.\n",
    "3. Evaluate the performance of your attack on non-private logistic regression models trained on a subset of the training set. Investigate how the performance varies with the size of the training set and the regularization parameter.\n",
    "4. Study the effectiveness of the DP-ERM approach implemented in Question 2 to mitigate the effectiveness of the membership inference attack.\n",
    "5. **(bonus)** In practice, if an attack is able to infer the membership of only a few data points with high confidence, it is enough to raise privacy concerns. Therefore, accuracy may not be the best metric to assess the success of a membership inference attack. Propose an alternative metric and use it to reevaluate the attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question 1 (objective perturbation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond output perturbation, the work of [Chaudhuri et al. (2011)](https://www.jmlr.org/papers/volume12/chaudhuri11a/chaudhuri11a.pdf) introduces a DP-ERM algorithm based on *objective perturbation*. Implement this algorithm for $\\ell_2$-regularized regression and compare the results with those of output perturbation.\n",
    "\n",
    "To optimize the perturbed objective, you may use a solver from `scipy.optimize.minimize` (such as L-BFGS, which is used by default by scikit-learn to fit logistic regression). You will need to implement a function that computes the objective value and the gradient at a given point: for this, you can adapt [the one used in scikit-learn for the non-perturbed objective](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf39bbdacd6ed713c00724f8f871d60370/sklearn/linear_model/_logistic.py#L84)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question 2 (inference attacks on richer models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membership inference attacks are especially effective on expressive models.\n",
    "\n",
    "1. Evaluate the performance of your membership inference attack on models richer than $\\ell_2$-regularized logistic regression, such as kernel SVM, random forests and neural networks (multi-layer perceptrons).\n",
    "2. Compute the empirical sensitivity of these models as done in Question 2. Compare the results across the different families of models. Is (empirical) sensitivity a good indicator of whether a model is prone to membership inference attacks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question 3 (more realistic membership inference attacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The membership inference attack implemented so far relies on two strong assumptions: \"white-box\" access to the target model and above all access to a separate dataset that follows the same distribution as the training data. This is unlikely to be the case in practice. Membership inference attacks of the literature aim to lift these assumptions by only relying on \"black-box\" access to the target model (where the attacker can only query the model without access to its parameters) and no additional data, see for instance the work of [Shokri et al. (2017)](https://www.cs.cornell.edu/~shmat/shmat_oak17.pdf) and [Carlini et al. (2021)](https://arxiv.org/pdf/2112.03570.pdf).\n",
    "\n",
    "Using ideas from the literature (such as those in the paper above), implement a more realistic membership inference attack and evaluate its performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
